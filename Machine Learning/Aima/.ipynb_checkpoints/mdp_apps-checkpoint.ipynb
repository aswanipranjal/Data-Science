{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLICATIONS OF MARKOV DECISION PROCESSES\n",
    "---\n",
    "In this notebook we will take a look at some indicative applications of markov decision processes. \n",
    "We will cover content from [`mdp.py`](https://github.com/aimacode/aima-python/blob/master/mdp.py), for chapter 17 of Stuart Russel's and Peter Norvig's book [*Artificial Intellignece: A Modern Approach*](http://aima.cs.berkeley.edu/).\n",
    "\n",
    "\n",
    "## CONTENTS\n",
    "- Simple MDPs\n",
    "\n",
    "\n",
    "## SIMPLE MDP\n",
    "\n",
    "Markov Decision Processes are formally described as processes that follow the Markov property which states that \"The future is independent of the past given the present\". \n",
    "MDPs formally describe environments for reinforcement learning and we assume that the environment is *fully observable*. \n",
    "Let us take a toy example MDP and solve it using the functions in `mdp.py`.\n",
    "This is a simple example adapted from a [similar problem](http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/MDP.pdf) by Dr. David Silver, tweaked to fit the limitations of the current functions.\n",
    "![title](images/mdp-b.png)\n",
    "\n",
    "Let's say you're a student attending lectures in a university.\n",
    "There are three lectures you need to attend on a given day.\n",
    "<br>\n",
    "Attending the first lecture gives you 4 points of reward.\n",
    "After the first lecture, you have a 0.6 probability to continue into the second one, yielding 6 more points of reward.\n",
    "But, with a probability of 0.4, you get distracted and start using Facebook instead and get a reward of -1.\n",
    "From then onwards, you really can't let go of Facebook and there's just a 0.1 probability that you will concentrate back on the lecture.\n",
    "<br>\n",
    "After the second lecture, you have an equal chance of attending the next lecture or just falling asleep.\n",
    "Falling asleep is the terminal state and yields you no reward, but continuing on to the final lecture gives you a big reward of 10 points.\n",
    "<br>\n",
    "From there on, you have a 40% chance of going to study and reach the terminal state, \n",
    "but a 60% chance of going to the pub with your friends instead. \n",
    "You end up drunk and don't know which lecture to attend, so you go to one of the lectures according to the probabilities given above.\n",
    "<br> \n",
    "We now have an outline of our stochastic environment and we need to maximize our reward by solving this MDP.\n",
    "<br>\n",
    "<br>\n",
    "We first have to define our Transition Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import *\n",
    "from notebook import psource, pseudocode\n",
    "\n",
    "'''t = {\n",
    "    'leisure': {\n",
    "                    'facebook': {'leisure':0.9, 'class1':0.1},\n",
    "                    'quit': {'leisure':0.1, 'class1':0.9},\n",
    "#                     'study': {},\n",
    "#                     'sleep': {},\n",
    "#                     'pub': {}\n",
    "               },\n",
    "    'class1': {\n",
    "                    'study': {'class2':0.6, 'leisure':0.4},\n",
    "                    'facebook': {'class2':0.4, 'leisure':0.6}\n",
    "              },\n",
    "    'class2': {\n",
    "                    'study': {'class3':0.5, 'end':0.5},\n",
    "                    'sleep': {'end':0.5, 'class3':0.5}\n",
    "              },\n",
    "    'class3': {\n",
    "                    'study': {'end':0.6, 'class1':0.08, 'class2':0.16, 'class3':0.16},\n",
    "                    'pub': {'end':0.4, 'class1':0.12, 'class2':0.24, 'class3':0.24}\n",
    "              },\n",
    "    'end': {}\n",
    "}'''\n",
    "\n",
    "init = 'class1'\n",
    "\n",
    "terminals = ['end']\n",
    "\n",
    "rewards = {\n",
    "    'class1': 4,\n",
    "    'class2': 6,\n",
    "    'class3': 10,\n",
    "    'leisure': -1,\n",
    "    'end': 0\n",
    "}\n",
    "\n",
    "t = {\n",
    "    'leisure': {\n",
    "                    'facebook': {'leisure':1},\n",
    "                    'quit': {'class1':0.9}\n",
    "               },\n",
    "    'class1': {\n",
    "                    'study': {'class2':1},\n",
    "                    'facebook': {'leisure':1}\n",
    "              },\n",
    "    'class2': {\n",
    "                    'study': {'class3':1},\n",
    "                    'sleep': {'end':1}\n",
    "              },\n",
    "    'class3': {\n",
    "                    'study': {'end':1},\n",
    "                    'pub': {'class1':0.2, 'class2':0.4, 'class3':0.4}\n",
    "              },\n",
    "    'end': {}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMDP(MDP):\n",
    "\n",
    "    def __init__(self, transition_matrix, rewards, terminals, init, gamma=.9):\n",
    "        # All possible actions.\n",
    "        actlist = []\n",
    "        for state in transition_matrix.keys():\n",
    "            actlist.extend(transition_matrix[state])\n",
    "        actlist = list(set(actlist))\n",
    "        print(actlist)\n",
    "\n",
    "        MDP.__init__(self, init, actlist, terminals=terminals, gamma=gamma)\n",
    "        self.t = transition_matrix\n",
    "        self.reward = rewards\n",
    "        for state in self.t:\n",
    "            self.states.add(state)\n",
    "\n",
    "    def T(self, state, action):\n",
    "        if action is None:\n",
    "            return [(0.0, state)]\n",
    "        else: \n",
    "            return [(prob, new_state) for new_state, prob in self.t[state][action].items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quit', 'study', 'sleep', 'pub', 'facebook']\n"
     ]
    }
   ],
   "source": [
    "our_mdp = CustomMDP(t, rewards, terminals, init, gamma=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'study'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-911a3dbacf52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvalue_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mour_mdp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Aman Deep Singh\\Documents\\Python\\Data Science\\Machine Learning\\Aima\\mdp.py\u001b[0m in \u001b[0;36mvalue_iteration\u001b[1;34m(mdp, epsilon)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             U1[s] = R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])\n\u001b[1;32m--> 129\u001b[1;33m                                         for a in mdp.actions(s)])\n\u001b[0m\u001b[0;32m    130\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mU1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdelta\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Aman Deep Singh\\Documents\\Python\\Data Science\\Machine Learning\\Aima\\mdp.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             U1[s] = R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])\n\u001b[1;32m--> 129\u001b[1;33m                                         for a in mdp.actions(s)])\n\u001b[0m\u001b[0;32m    130\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mU1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdelta\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-f213a59c42d8>\u001b[0m in \u001b[0;36mT\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'study'"
     ]
    }
   ],
   "source": [
    "value_iteration(our_mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
