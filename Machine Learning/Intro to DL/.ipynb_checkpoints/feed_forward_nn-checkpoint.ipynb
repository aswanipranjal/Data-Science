{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.seterr(over='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class neural_network():\n",
    "    def __init__(self):\n",
    "        np.random.seed(1)\n",
    "        self.weights = []      # dict to hold weights\n",
    "        self.num_layers = 1    # initial number of layers\n",
    "        self.adjustments = []  # dict to hold adjustments\n",
    "    \n",
    "    def add_layer(self, shape):\n",
    "        self.weights[self.num_layers] = np.vstack((2 * np.random.random(shape) - 1, 2 * np.random.random((1, shape[1])) - 1))\n",
    "        self.adjustments[self.num_layers] = np.zeros(shape)\n",
    "        self.num_layers += 1\n",
    "        \n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def predict(self, data):\n",
    "        for layer in range(1, self.num_layers + 1):\n",
    "            data = np.dot(data, self.weights[layer - 1][:, :-1]) + self.weights[layer - 1][:, -1] # + self.biases[layer]\n",
    "            data = self.__sigmoid(data)\n",
    "        return data\n",
    "    \n",
    "    def __forward_propagate(self, data):\n",
    "        activation_values = {}\n",
    "        activation_values[1] = data\n",
    "        for layer in range(2, self.num_layer+1):\n",
    "            data = np.dot(data.T, self.weights[layer - 1][:-1, :]) + self.weights[layer - 1][-1, :].T # + self.biases[layer]\n",
    "            data = self.__sigmoid(data).T\n",
    "            activation_values[layer] = data\n",
    "        return activation_values\n",
    "    \n",
    "    def simple_error(self, outputs, targets):\n",
    "        return targets - outputs\n",
    "    \n",
    "    def sum_squared_error(self, outputs, targets):\n",
    "        return 0.5 * np.mean(np.sum(np.power(outputs - targets, 2), axis=1))\n",
    "    \n",
    "    def __back_propagate(self, output, target):\n",
    "        deltas = {}\n",
    "        deltas[self.num_layers] = output[self.num_layers] - target\n",
    "        \n",
    "        # delta of hidden layers\n",
    "        for layer in reversed(range(2, self.num_layers)):\n",
    "            a_val = output[layer]\n",
    "            weights = self.weights[layer][:-1, :]\n",
    "            prev_deltas = deltas[layer + 1]\n",
    "            deltas[layer] = np.multiply(np.dot(weights, prev_deltas), self.__sigmoid_derivative(a_val))\n",
    "            \n",
    "        for layer in range(1, self.num_layers):\n",
    "            self.adjustments[layer] += np.dot(deltas[layer + 1], output[layer].T).T\n",
    "            \n",
    "    def __gradient_descent(self, batch_size, learning_rate):\n",
    "        for layer in range(1, self.num_layers):\n",
    "            partial_d = (1 / batch_size) * self.adjustments[layer]\n",
    "            self.weights[layer][:-1, :] += learning_rate * -partial_d\n",
    "            self.weights[layer][-1, :] += learning_rate * 1e-3 * -partial_d[-1, :]\n",
    "            \n",
    "    def train(self, inputs, targets, num_epochs, learning_rate=1, stop_accuracy=1e-5):\n",
    "        error = []\n",
    "        for iteration in range(num_epochs):\n",
    "            for i in range(len(inputs)):\n",
    "                x = inputs[i]\n",
    "                y = targets[i]\n",
    "                output = self.__forward_propagate(x)\n",
    "                \n",
    "                loss = self.sum_squared_error(output[self.num_layers], y)\n",
    "                error.append(loss)\n",
    "                \n",
    "                self.__back_propagate(output, y)\n",
    "                \n",
    "            self.__gradient_descent(i, learning_rate)\n",
    "            \n",
    "            if np.mean(error[-(i + 1):]) < stop_accuracy and iteration > 0:\n",
    "                break\n",
    "                \n",
    "        return (np.asarray(error), iteration + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
