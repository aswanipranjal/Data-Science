{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8563"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = open('kafka.txt',  'r').read()\n",
    "# Temporarily opening 'potha.txt'\n",
    "data = open('potha.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'(': 0, 'i': 1, 'x': 2, 'g': 3, 'M': 4, 'T': 5, 'e': 6, 'z': 7, 'v': 8, 'u': 9, 'f': 10, '-': 11, 'O': 12, 'n': 13, 'A': 14, \"'\": 15, 's': 16, ')': 17, 'R': 18, 'y': 19, 'G': 20, 'p': 21, 'a': 22, ' ': 23, 'm': 24, 'r': 25, 'S': 26, 'D': 27, 'w': 28, ',': 29, 'c': 30, '.': 31, 't': 32, 'I': 33, 'l': 34, 'j': 35, 'b': 36, 'o': 37, 'k': 38, 'h': 39, 'q': 40, 'E': 41, '\\n': 42, 'd': 43}\n",
      "{0: '(', 1: 'i', 2: 'x', 3: 'g', 4: 'M', 5: 'T', 6: 'e', 7: 'z', 8: 'v', 9: 'u', 10: 'f', 11: '-', 12: 'O', 13: 'n', 14: 'A', 15: \"'\", 16: 's', 17: ')', 18: 'R', 19: 'y', 20: 'G', 21: 'p', 22: 'a', 23: ' ', 24: 'm', 25: 'r', 26: 'S', 27: 'D', 28: 'w', 29: ',', 30: 'c', 31: '.', 32: 't', 33: 'I', 34: 'l', 35: 'j', 36: 'b', 37: 'o', 38: 'k', 39: 'h', 40: 'q', 41: 'E', 42: '\\n', 43: 'd'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i, ch in enumerate(chars)}\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "wxh = np.random.randn(hidden_size, vocab_size) * 0.01 # input to hidden state\n",
    "whh = np.random.randn(hidden_size, hidden_size) * 0.01 # hidden state to next hidden state\n",
    "why = np.random.randn(vocab_size, hidden_size) * 0.01 # hidden state to output state\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(inputs, targets, hprev):\n",
    "    # hprev is the hidden state from previous time step\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    # xs stores the one-hot encoded values of the input characters for each of the 25 time steps\n",
    "    # hs stores the hidden state ouptuts\n",
    "    # ys stores the target values\n",
    "    # ps stores the outputs of ys and converts them to normalized probabilities for chars\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    # init loss\n",
    "    loss = 0\n",
    "    \n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1))\n",
    "        xs[t][inputs[t]] = 1 # inside that t-th input we use\n",
    "        hs[t] = np.tanh(np.dot(wxh, xs[t]) + np.dot(whh, hs[t-1]) + bh)\n",
    "        ys[t] = np.dot(why, hs[t]) + by\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "        loss += -np.log(ps[t][targets[t], 0]) # softmax cross-entropy function\n",
    "        \n",
    "    # backward pass\n",
    "    dwxh, dwhh, dwhy = np.zeros_like(wxh), np.zeros_like(whh), np.zeros_like(why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    \n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # output probabilities\n",
    "        dy = np.copy(ps[t])\n",
    "        # derive our first gradient\n",
    "        dy[targets[t]] -= 1 # backprop into y\n",
    "        # compute output gradient - output times hidden states transpose\n",
    "        dwhy += np.dot(dy, hs[t].T)\n",
    "        # derivative of output bias\n",
    "        dby += dy\n",
    "        # backpropagation\n",
    "        dh = np.dot(why.T, dy) + dhnext      # backpropagate into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh     # backpropagate througn tanh\n",
    "        dbh += dhraw                         # derivative of hidden bias\n",
    "        dwxh += np.dot(dhraw, xs[t].T)       # derivative of input layer to the hidden layer\n",
    "        dwhh += np.dot(dhraw, hs[t-1].T)     # derivative of hidden layer to the hidden layer\n",
    "        dhnext = np.dot(whh.T, dhraw)\n",
    "        \n",
    "    for dparam in [dwxh, dwhh, dwhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)   # clip to mitigate exploding gradients\n",
    "        \n",
    "    return loss, dwxh, dwhh, dwhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "    h is the memory state\n",
    "    seed_ix is the seed letter for the first time step\n",
    "    n is how many characters to predict\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    \n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(wxh, x) + np.dot(whh, h) + bh)\n",
    "        y = np.dot(why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "        \n",
    "    txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "    print(txt)\n",
    "    \n",
    "# Reset RNN memory\n",
    "# hprev = np.zeros((hidden_size, 1))\n",
    "# predict the next 200 characters given 'a'\n",
    "# sample(hprev, char_to_ix['a'], 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "1. Feed the network with portion of the file. Size of chunk is seq_length\n",
    "2. Use the loss function to: <br>\n",
    "    a. Do forward pass to calculate all parameters for the model for a given input and target pairs <br>\n",
    "    b. Do backward pass to calculate all gradients <br>\n",
    "3. Print a sentence from a random seed using the parameters of the network\n",
    "4. Update the model  using the Adaptive Gradient technique Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs  [33, 23, 22, 24, 23, 22, 21, 21, 34, 19, 1, 13, 3, 23, 10, 37, 25, 23, 10, 1, 13, 22, 13, 30, 1]\n",
      "Targets  [23, 22, 24, 23, 22, 21, 21, 34, 19, 1, 13, 3, 23, 10, 37, 25, 23, 10, 1, 13, 22, 13, 30, 1, 22]\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print(\"Inputs \", inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print(\"Targets \", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter  0 loss:  94.6047307012\n",
      "zc)chcvfi ,Ekzy(ayfoMh'd-qm.ztugmT.w,\n",
      "\n",
      "fIEMsigSArn-.-iq'oyErkSOduOg)DrOR(uj lkjzmubw w aaMm\n",
      "-orhIDD,uAptxmOobd,uuitjpy)(ee ,Sxh-aaOceAMSlTzbIfcszu-ylu(xrDjMGrul'Moi -A'bxOpOoSqT(MaE-EfzqgEIMaGb-jmzsSi\n",
      "Iter  1000 loss:  77.7907489884\n",
      "int ard of tin tapl atary ass'osl lyaml howtave ce limrThels aptiom rtrt Te s ly ny or mo ited the mu te walyeataTvuread to af ilt manapddos we. bn toarurede n. Iops oh biystf xOere anpte. janbcininh \n",
      "Iter  2000 loss:  64.173886564\n",
      "tsenTtars inprarbth irswoedem anceerk coapd ner impry ti'e foy I to faldisers molpanlnang ard auchiilo I th torhge reotht anlrssom ty I cya\n",
      "d.\n",
      "I mmortus bud ni canisearind tay lyincearl antjawinl peng\n",
      "Iter  3000 loss:  56.8138625091\n",
      "or if in bepernt le a fosicawlsiwlegn pok. I forkey nhard wived fo coacincilltiln on de hore to y. Ears jent if foutore Mas taxg oud woteymay whent. I go diuppt imps wis ancepreesds on mut ely dorenan\n",
      "Iter  4000 loss:  52.5356726673\n",
      "a finker to pfoble nat. I wavpisting inned tiare, I thisure ablle derseiningtle finxincore unerxiciand helna with afling letiilceciingitusm and te lyd beadgoin cam is imsay so\n",
      " Dury leigs. hhe rtore v\n",
      "Iter  5000 loss:  49.7757351826\n",
      "matpiscins af ta jult sorlimve it buine,dEanld wte rot owite hal  meacdy ancerso bicand gham dond I actiimpert. I I peandictiinive I arncintile geald withe fon ues quleicert le. I am kmy rot incunigl \n",
      "Iter  6000 loss:  47.8525839299\n",
      "m ta phearc but is cont in as I thablly af br on nerowt unargshis geset on mys.\n",
      "I chaac and mesounitilavy the oplecathne so knorins fulsine.\n",
      "\n",
      "I ane had ou ly woull in corared applrarning buile tuthik \n",
      "Iter  7000 loss:  46.3939338391\n",
      "iape fit ber ot retoso fuolSidald naalny hat wart be ant ath wsowe an setaneendos in marart fithe filples intos moa byeedd dikn afse ssufilplytyimpe maysove tore io curaurs am suicinang.\n",
      "Sinly isk mo \n",
      "Iter  8000 loss:  45.2017998602\n",
      "protlyite put tu got us thidarn guve meptre hounk so beatd tut be firncoul are whhh pamay wor I oap I da ledso luthe cho geant my raid me strerichidt in the iesees. I arl knowithe awcerdaninigT ve wol\n",
      "Iter  9000 loss:  44.1075918445\n",
      "s ipfest ous thor bebe ceracenve lent uficille gat I kningd ans mous realn feps het mese peort it laut  uner in meerinigl con meamy an bene. I ampapling fure mes fure ion offrerigh collle re tole moac\n",
      "Iter  10000 loss:  43.2389608951\n",
      "anlconecand an int kiseport arpsot, ofppeys emame ase of thes afisthly dold anstos sote of imy I cears wance so her. I gach waoge on mac I ther ares wo porest to hom on, I abuc sttatawinge bet to ans \n",
      "Iter  11000 loss:  42.1922550652\n",
      "orsuipaly  ullecos to to myeront wikling beses of uraleg to to leparnt is to ca deth docher ollo poplely mer I orprecpmous afp to for aply opl imprarens hours ous mimarte lut sels if onk Sorts nonvert\n",
      "Iter  12000 loss:  41.2916139687\n",
      "ppomant in whar arnd. An ors uret sow iffore to vet is are to a vers. Ean to sk of wilp letes, Ours morejulitel in junler hout oles ablly finanse fled ably hant io vers un hill br olparcint apley too \n",
      "Iter  13000 loss:  40.3589229074\n",
      "utith wigs apptose on are teece ssimacten n eng owl. \n",
      "I wactie hablico fininc or a courses to fors on puoll so and a st und rut ure, is dakrito dos tot iffeeme ferso s metary meer ina pts mo wly just \n",
      "Iter  14000 loss:  39.5782049895\n",
      "kully gutlo fice of sthild dosedo veres fip tansian am for aple copjusturstins rortit lete. I hald I wared gear impetts mut inpble pary a in inceed buid. will br furerething is har bo tourncentty ar s\n",
      "Iter  15000 loss:  39.0438708789\n",
      " I am arews filearnntuncor wor ret a beobe and ande I haa a nete goule I kne pest and and but bear buig the am ans anensing trifille on knoty are bebly filese. I whatte damen to(meer on are is a hot. \n",
      "Iter  16000 loss:  38.5025149869\n",
      "l k lot fiflem aigatit lears in prowided in sko fisorew a ally tant I theprkil, I meecable t ukit builyg ty wolrk wield cuurk of withe. A for ares and and contepl worn praus oblengoty olplane no sning\n",
      "Iter  17000 loss:  38.0009477182\n",
      " tat Mritresey impreskay -sible notat wemance will am lyts. Msy thek hant will be inamcent on cand to car a for bre my sileen afle hel ley sech. I gearning ifferys and peering.\n",
      "Ek payses on canting in\n",
      "Iter  18000 loss:  37.6316313572\n",
      "g I meethon whonh able to knhildanvielsof wither dine and the reatmy knicep in thove af ta be at intruinal--ning bet off in whithem nitely wam if I wint I havengeptos of tama st in wham afile bears th\n",
      "Iter  19000 loss:  37.2130791593\n",
      "w ont I wame and cerknoo concepts my nood and gerys. I wantes I thenead ly. I wig to poow me trearnings thithee ancouseplemance fini's fhem a ind anancint. Eance and a to courthamicald an anont in thi\n",
      "Iter  20000 loss:  36.9133304764\n",
      "d onco cear ace to prouthoufe wors and alrantde.\n",
      "I wilp ly who bilt with a psoments maed and and cemt my reor furearning ono hem ans and an, I aca bhive hoffentals th the havad a dancen a lefingels io\n",
      "Iter  21000 loss:  36.6571853435\n",
      " de for buce wous to I anenithith to ippose to proepse. Mo payd fo knovathihalda nse thef fo coop fre a lolly these.be holve for aplous anace resialn buile and ithepts anse futho lears inh cert to pay\n",
      "Iter  22000 loss:  36.26418102\n",
      "alsemaqarcedt ancceprarentisuls hove pay my mo fo\n",
      "I jentesse. 's ceent thers om the co to finance and in mo on sequins pestufidaw matiinaid bede tace my tomey aplot ar woulve tath fhise leicthical or \n",
      "Iter  23000 loss:  35.9951737522\n",
      "ng happ leered to here ho abecen ants in but learly striod os ont on my my resty wigh of on the inpy of mang I there my om omt rearnita, cent ot fincts is marsedegs in the tatpistely my coppaight-to v\n",
      "Iter  24000 loss:  35.4540004891\n",
      " le fee ablint dewenso fut wor are my buinalg sort ink work lantict and tat hemernstures I bes stiny cotpeictern a dantern ande to councise leally wact to incent dearn wied I wills cor aus theredaic v\n",
      "Iter  25000 loss:  35.0532575565\n",
      " ho klay searning and theme to fore aldem in des work am. Earty in learnotting thepre an sutter breres and wien llion orce se blechent to pael leald witlezese. fing lenetsinturevy thice of rat conprow\n",
      "Iter  26000 loss:  34.5276283728\n",
      " oppthere to paur who d am in the knicept filg. As mem to luicant tor my fime s lut to fime and I conente, do wore a korich tor intor omt is have ataye in demys io cotes eristily mes sears opte pue fo\n",
      "Iter  27000 loss:  34.0841941248\n",
      " bully matdee septe not fing oppthours onc maracthod hithe.\n",
      "I help inces seles ning the ubl at to ceytseig be a zay free popte. Duty ours a lot an imprene will help but co cerkily ant wing resutho buc\n",
      "Iter  28000 loss:  33.9089138625\n",
      "orve to ctore a be'd of usuicant to melo goeld and justice worke wolld and alro ceremince jent on kartidy.\n",
      "\n",
      "Evemy hald and in cat atheres noipar sor iprantould merreres or courcho coursin the mertice \n",
      "Iter  29000 loss:  33.528885763\n",
      " whettors in be unichie ald biaur and to marale I winawly wata bechente a very da gotacertifile biernd on cuuy proist I gea acictust oI expxmaed in cerenothald buct te orply mye a letequdearntond. Esu\n",
      "Iter  30000 loss:  33.2592053176\n",
      "uct ha pacien newathe derod afly have cextay my, butreacsince co coutt it prowly gut im senicine have a seplotf of luep.\n",
      "\n",
      "I aur gemy nood aplimand I knowleddand ang in dane's. The whout Dyciant I part\n",
      "Iter  31000 loss:  33.0887226864\n",
      "ote. Earing ifle kar I kes amerg iy daceqy sontapt to mancethese. I obve posearnithh will ge hele oncigedid knourgilytils and I hors cal ink here of dere jusiuch hed aal I dmercourettor restacte ac op\n",
      "Iter  32000 loss:  32.8712130951\n",
      "ly whar are fiqe. I mabe mensourlly my gocert fible conence. I concest in wores aed for a io se and an. denobt int at uare deats but in cerearc hard an andey strents to bo I wot oblemis ced so da and \n",
      "Iter  33000 loss:  32.7290977211\n",
      " whow ueatbe cerstacoustile s ont to pay junewly wid honter, offiuthits. I am ame finanin thebcead cetttrese oflrinat for aully winamint-, do peemizin tut ak olpryaly of rad leally waid aly. I justica\n",
      "Iter  34000 loss:  32.6497048166\n",
      "oppill but very lytlyo suill cotpit ulaly hrarr on dande lesed filpdaid dears of thom anse. Our aile to dobee abe aty wot on chand tave arssun, I wiche and I deby rut opprearning icvet of rachill. St \n",
      "Iter  35000 loss:  32.4464493697\n",
      " and ace reptiny beteqs st kertowhiabl dat. Alr in buil and maccently starntuth inave aplod and wor hopploves in am sute the are mo dote juning thert I whep uave oble mutersin cence it butice puuly in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter  36000 loss:  32.3064713208\n",
      "ompang to(wume the have I wimance andy stroc and maks builsher justich fisels in thor hamy ald all meceacskidit ly who data dery orf iel ainciltedo deable so proall inceust I wata bie'r lyebly hegc ne\n",
      "Iter  37000 loss:  32.033277103\n",
      ". As sopsolt there text tor ar cowidghert soln improve of pase to I hebe hel beeld and a hingw I disy finance dont I juntold to pherns and wiel concerebly have a loincent with thees fon appledy getolh\n",
      "Iter  38000 loss:  31.6800282194\n",
      "ise. I sk ullly woul hes net trient te semercolno nog of ly ulle to beche pay too tor it im my finaps inance to so coreed of dat om wippley. I hotherntizer in se luts to sot ore able ets fees. Simuse \n",
      "Iter  39000 loss:  31.2924648252\n",
      " jert-ttistiwit fit court of knother. I have an aplomes a boricthh gortith improve roursett, I goureabl, I a dath ins fileess to pay rearning lrinancare will difl in tare mo I knand biowcom conte and \n",
      "Iter  40000 loss:  30.814233818\n",
      "ares and in aress to cerearning piove vese wita a demess turemave gewlyes. \n",
      "gicaccer. I omp. I buther bring tre a in catical deet of sce andod and and andens. Eares in wauls an, I har puise oun my fic\n",
      "Iter  41000 loss:  30.7702860039\n",
      "f tho is fors. So in thit sk'rsing on the incos olle and dereod or racheme wle ablice futs ushersin tat tropley.\n",
      "\n",
      "Evethifre omprertett dete freptrywwom) usuly my arning have and a caut to depang buila\n",
      "Iter  42000 loss:  30.5121928598\n",
      "ls. uth wels andearning brabl ont Destaly will I cuthes and I pay on mpartificels onh go da peigulle perctoid hleeg financese learning to cours shel it knowack of cont freprelals alsiniche hogt. eos g\n",
      "Iter  43000 loss:  30.4242007806\n",
      "ent-tit it for isplisee and to har be and seine.\n",
      "\n",
      "Reead learn nucho fine to compuiave ryes. Ar hource to therikave. I war I sunhilserese in canancle ole har emaredy there com. unally ge, mo donces in \n",
      "Iter  44000 loss:  30.278075765\n",
      " isiprsos or relobh heut ers plopeept I da any respporing ffile buich.\n",
      "\n",
      "ES io dand inh resmancents to sk in helkieptly whit wimilsesuig con aurse finabacachitlenathint a pet on dnang in the hote conje\n",
      "Iter  45000 loss:  30.0041586496\n",
      "d alplying on the knyolne sot. I da know ining to gearning off wod Eawimastist. Dute isterning work olltificatinilledey sersettisult oe ereen. So coult wouse I peopls ithrovuitad and have ablid bicalt\n",
      "Iter  46000 loss:  29.9254387713\n",
      "apply to so leally work hatt a datas justur and be my recelt to cours for ale and here but I impriard of I am ontipe and the a poustsingwen aes but out igutise appoly just I ork hers ohl mozoffenenty \n",
      "Iter  47000 loss:  29.897057642\n",
      "o pamizs it om if a amy a stels and pany tow me a vemisenees on my off ouve daringd ansoult to the mearcuilld unh har certuneens on machithe cow ment, I om ares gething for ha compreskily an to with a\n",
      "Iter  48000 loss:  29.7046572223\n",
      "iltho dgace alle the thes fut fromist ont resod my compently me to frovesy whor anyilg to fure helsereysepjer. I orv at our am mactly willd uning to coure hand Io a setable butid thio courts in readde\n",
      "Iter  49000 loss:  29.81472094\n",
      "eese jullom I will bol go bethod iurt wifle wota be thichur of pente tot improve rearcaccorte so I am in thisply huld th proese to pay my knoo I am am e'no, Sine I da in thepring for gocsinarant, I da\n",
      "Iter  50000 loss:  29.5945017976\n",
      "ly to kepsepselngento,  und ont I ank my forkill. I a apllyo foblegs myos brety coept to pay sowicich thik dey toh implay who the useneres in court to wille ences. but blicactho very relor skile.I at \n",
      "Iter  51000 loss:  29.2428919386\n",
      "en for is. ponabecheve bes witidcent in the ulle with the paifa, bur hild be ranty toy soel offfoud hard deth mansed stso fergs or data ninicely dufle t ertholl just I hey courcs in, pote on dable I g\n",
      "Iter  52000 loss:  28.9403911228\n",
      " tut resume lesern an my tomenications febioungosit leabl invense for deatand to conim. I kerses hele ald ans a skill just vere justhin chit le concese wing tor herts of mactailed. I hlabhe and tut in\n",
      "Iter  53000 loss:  28.4970467299\n",
      "poy cancers. I gotheros of im. aursin courts pealy held ant st howeedd for ta chied an taf I am ares ifmery my simatat in the panicg dimpine.\n",
      "I hild exphepple I gott apse buly wane dable to course. Ou\n",
      "Iter  54000 loss:  28.3420621007\n",
      " prolvenichinen. Gives and tfia arncersin are se and knowwor dowe no cand use. My rearning to to have a tom a noo danishle and mand hand tor'no deakly wilt be and rackiont to pacistule aint out unicun\n",
      "Iter  55000 loss:  28.2083585544\n",
      "y noo doppinancaut my projecens imars ning I da know mant to interes in the plexplly wawd anewaid.\n",
      "\n",
      "I am mave onh help are and aspsese mes pochill bo people fiee\n",
      "ley wro goald to coplo coneed quiente \n",
      "Iter  56000 loss:  28.2925543087\n",
      "d fort of gefiintere. Ald I good hed but raciettting hat am on my neopley. I amp invercanes. ontuur my sinteme comest in I gonted of douse my suald on diffinted to cours to ippfith paid enctut om unab\n",
      "Iter  57000 loss:  28.1571154072\n",
      "we. Ald I be with wegor leays becath vestald thinathow to geskideccille to conemacsol, mace ffiorcousttro ereen nest wingt an putly in dow secest will be able to the meraninn fusthure a lethith certar\n",
      "Iter  58000 loss:  27.9253491578\n",
      "ut of puom cacher arse in the sequio becens. I I be know and my puinacend and the in buishid a vemaysture of buth orplichel apl macs couret-setancem and a lentod opply but and and tawings ingon ceadgo\n",
      "Iter  59000 loss:  27.7668043917\n",
      ". I am 's rearningh hele the but in tam iploppot on maching. Earning will good pppoptistures and a de tor ulle a de ichem in dance dom I kncuncel desing of theep oppty rat I lealle thior at riskiorcou\n",
      "Iter  60000 loss:  27.9017204164\n",
      "s hele have partining leicave angencer I igh I get I toments I ge. I og om intly mede afle a pasiars ringow fhe apligeeng of fuill but hard and I deally mpemand ton tkit by bh ved fos fhout lote of an\n",
      "Iter  61000 loss:  27.8583515227\n",
      "out be thippring be hele onw uliate se uneess on or me any ael hat I wate in dear and file. Our of land in my fall ane my imfulve gote the them)s in butiels and plem)newledy anden idaly whifhe I willl\n",
      "Iter  62000 loss:  27.7552855213\n",
      "nt\n",
      "Stfwim gecte ran qe.\n",
      "\n",
      "Reltior byte hente courts on applintte to work dyestrentte to couthere al for ines ifhey bleed bee tomeds and wills with necuita a doe.AS res I so verid. I have built a gojene\n",
      "Iter  63000 loss:  27.7152408639\n",
      " high tuts seell tho gocents nojeed leatinscaull in tailced I da copseeptly tang a lios bleceposo tom erculian, I rute ol' no to kintold to mame de han I fort to a leresininghor leatiny ceskinicgh imp\n",
      "Iter  64000 loss:  27.3280565297\n",
      "remand ande in the the pachina, I hot uredy but waur penow wadd af, hey jut to improve. I who court improve doo dof ananctut dath ince for and a dearntar. I whilse to paos opplly use ary rocebtich too\n",
      "Iter  65000 loss:  27.1122597477\n",
      "ing cors in maths are ofved I past to coursaly da d anacscurad dy ca pet cetass cug omprojechert in thepr aplom. I way wat intmabl bet to vog. I emeard an or recheptly of day. I will be able to profic\n",
      "Iter  66000 loss:  26.7615690981\n",
      "ewore in my sinest forenso stuct rotin sumle to learning wroseny may dojeededy so prote for fith ichertic thid knowlege learning ipve treald I paptly niobl are help and my thersenenance and I knoon ui\n",
      "Iter  67000 loss:  26.522653346\n",
      "y omproal anden leenn ont to imversinith ish doo for datad I waw widd a pith bathourchit ercaly cemes ive abyca and I here mont to do I am surily projecttisel fo pay oppry lyking. Earccing of mispemy \n",
      "Iter  68000 loss:  26.5069284842\n",
      " lend of iprenty to the sheptio, I get. Acs skirand the flie will be hard on gulpiny them to covredang to beaild and will gefise, I the able I wayk pantisthowleg finand ables sning to learnend thiops \n",
      "Iter  69000 loss:  26.4086133632\n",
      " dho daut becemte incenend. I hbik doure. I winl a loy me a dourseptor. I uld alpiars. Siprqy olle tonconts ancente to coremizata nentict. Moop mysimferytustily stly mysin macheptly ot Mos co the topr\n",
      "Iter  70000 loss:  26.4183817862\n",
      "es buil alse. Aly ink I upply whie. Alrow wat estsof ime so sturectiel lot. I have arno, hor ar ofles. I han ay ompole wirncizmisear. I have deawdy be ay mpacicachimit st hotproal hinleg to harp becer\n",
      "Iter  71000 loss:  26.2573532053\n",
      "nte sod a jent to comprowwly thes ave ifficel and I wint thes cettle her bringod tor hars ty mave but wit lakl to puema,, wante andod ay mysproal aid.\n",
      "\n",
      "I am apply suls. So mount if fie cours aursete j\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter  72000 loss:  26.0883787595\n",
      "yingody giolle wild be higaled.Als ange herp'e sot I the ha cho kese whe perty or or's marenont th too buct so sinveys of mable ush it hota ctat of macke to pay leare to deame to pay of the theme.\n",
      "Sce\n",
      "Iter  73000 loss:  26.273218489\n",
      "nectaide and sure my so wome to icacly have bifiel it skileled st onh ga cote recaith solpdemance and I really wable will my restifllo got to geoplor anconte to viols futso in nowt I wauthout will be \n",
      "Iter  74000 loss:  26.1959837387\n",
      "orely wita cearnithtus hle knowgo pery of tom wow dearncepsof oppty rot. I improve thes in pequile so pertats wing the mact but ot concoun improve me isem and and mete of data stsin the toplouse buth \n",
      "Iter  75000 loss:  26.1843728715\n",
      "sturentians ave my roar in thin al is. I I ko compriof learnttisel in the futm and thiplr a dote st. I data in a rask oflSage ours antly there I wage fill I dgoals and hat a loictury paes. I our buth \n",
      "Iter  76000 loss:  26.1767633013\n",
      "mand temistjwikve doul a loo futh condayds nondade on many waid searse ancesentith incos all meess. wewledd.\n",
      "\n",
      "ISuread scerns igeleso se teer so to heee and the feese so thita bho I aco im. Motmy the w\n",
      "Iter  77000 loss:  25.7576581928\n",
      "to vave for my theruifills ne's. So ext and a caut ar ons iprente to whacese of masenow ale umaed on coull apply blical a ar I am projecoly suskinity letye to pases to wly heled anicald on way. I wat \n",
      "Iter  78000 loss:  25.4557660785\n",
      "ey umver sowced beedg to good appersint amte goeely despare. I oche know bes and to chor concette iffio desengo.\n",
      "\n",
      "unsele thet ord jest in cerearning to the conange tos ugh I wausing cettid willedgouse\n",
      "Iter  79000 loss:  25.1595896859\n",
      "o dance and an mes if learning who verovitinthilg concers. I omeresany willcenttt in the pangh justiln to beke.\n",
      "\n",
      "Reestte, Af, is hat I cerkilvedd and I wous wal contmyse. I se finalThe tan obliy s app\n",
      "Iter  80000 loss:  25.12037822\n",
      "d are to core ullittt entily hat ak sefindest exd aed an to m mementipts in many wamy winve to omp. to impeing iend it I am sunh will defone rosiches an ap is fica to vetiand to deyencinable rerts on \n",
      "Iter  81000 loss:  25.1097232589\n",
      "omace, but anichs but anse. I kneod rxpels ont applyiwit-, expe with at furtacstuit pemand and alro womest in con applrinaw ont I wing to donenest and I wace to vie intlropprey solle tom rearnicable r\n",
      "Iter  82000 loss:  25.0102752117\n",
      "ngensourghilg I pemens tot on the finablinind alrening to doop helte I gesnend for ar, I get I poos note merifhor futise hores and to learn urt I wale dowle and in cha chatience aly ofld a very past i\n",
      "Iter  83000 loss:  24.863876238\n",
      "oald a chitar monts a lot unicald antitild but ay my parthing in theepsepfiet stlio doppring in the toput ruaces inc.Als int a nearning thicte thase concestsiorse. Ous wing ice to offeres furversetss.\n",
      "Iter  84000 loss:  24.92525109\n",
      "lso projectes I cofficurse in tharelon wawd ane to rest isprompactiently thit on appothinaurnd thipltit I sear, I a compery thexp learning rest and just and thippisy a very my orseelly offeed and an l\n",
      "Iter  85000 loss:  24.8884738279\n",
      "y raillse fol apl gachint so knetyintell an my blicutl in bely wayced for course to to wore partilncepts in but thy imperg this nance the compronkere reseance to to pause on machigleqe of seatan, I we\n",
      "Iter  86000 loss:  24.9005371445\n",
      " kentes and to pay my cea bet ents to course fors in bugh rearning on machigh of ipplily go, I get. Ald ipxlacand bee to viold. I helt on gee.\n",
      "\n",
      "I rare of inat wing becelachith th and aud ins and with \n",
      "Iter  87000 loss:  24.9291868076\n",
      "caninted to meecs tremany in my fitter so conte will be gecill hard an incerint, Sore ghils alruild.\n",
      "I am anemy rebly the ruille anl thy out my thed knty offiopp myesk to pronar.\n",
      "\n",
      "Alntilntilg. I am co\n",
      "Iter  88000 loss:  24.915268035\n",
      "is ficy thimaccine countit couss and ugh my ary the isere to bathin cant thod aplopte pestares in thare bees. Alse goicheply maknitled and olp mo. I Impremerestat to petye mand I knore hat ithopery he\n",
      "Iter  89000 loss:  24.8935742399\n",
      "for a collowithice finceys intuse buid. At me bot a dowad luations in machint and a dise I gote. I havl, I har hak it apr at of put mise for ume to filpt to impwey nowted hented on mace but int and us\n",
      "Iter  90000 loss:  24.5546767477\n",
      "apl mopse. I hate of day to to fursesiniche int is tho fhere ic cersents. As homes and or the ruat-sinant the hath olseatints ans a goiand will hit ences nos I would be hert of pertto tomentullly helt\n",
      "Iter  91000 loss:  24.2577273038\n",
      "ily am in the tlyor mys har I shern a veridgfunita highe to courncer four deerses. Sinabuill but wrachine on seer in realily gequises on maks of pemts in wheta ans and the chither to imves sorcered an\n",
      "Iter  92000 loss:  24.0644212065\n",
      "y with ars heel and I wanc to firarning, une pat I wild usint aur rytay w exk on my furearning fe on me foring cont ippeleaddet. I have ablid lyall anemace. I watiegs of rearn leabe gecers tom and pan\n",
      "Iter  93000 loss:  23.8636033978\n",
      "to very luto vertit course. I win as gexly julle ut cale mond hot cat buciceld and and macese will do desenders fficult strustust any kny ghe but wil chothencenarend ffictly iffere andodprears ottt op\n",
      "Iter  94000 loss:  23.9168690796\n",
      "olle. Arsurearning fid becancest lesurectiss if is hiolly the course. Al my preally womtere a let ertaly jultolvowle mer fors to pacaure ros off gentat, and dar too prowhilpted the moacholpitt the sur\n",
      "Iter  95000 loss:  23.7833195375\n",
      "io sese or schepr ans an myssposo dexped tot orve mare res and maciors intuscullising carp aly me a lot tomdy an helt yestmise and mond of deme fills eres. The sesinecols and alro with mere pay full, \n",
      "Iter  96000 loss:  23.5569127153\n",
      "iolle for ary nalpidy apl a canty turut win able I or daield ont iselom ande lears hencant sintinal fine. I wane daud my or mmapldy whonathine ald and als sials on apfly my incaydy couse of it dournco\n",
      "Iter  97000 loss:  23.6061704535\n",
      "is coulnd not is coject. Myserts of machiglere hag on macacstily to pueldly wlog sen. I a certiny reellicurl my cay fficts in reskiniln ant so I akn inntiorse. Our cores of know ulfle to lack of pospi\n",
      "Iter  98000 loss:  23.5454474725\n",
      "anacth is. Silrcemantit using. I out urtuiso insurest in the pan's. Al opt mhereichal a loalses for courcollousg unig anted cenced. Als iplety womes and it. I ughopply am a lege will ben expjent-cscif\n",
      "Iter  99000 loss:  23.539881127\n",
      "pplets in ly mathintthor, for aplo butle gued oun myeon cisearning will my rentre pus nand ont reare on tan and olr my racical y tinable to fille woll thes ave buinderand on ca Dio se to prow tat eran\n",
      "Iter  100000 loss:  23.7129409629\n",
      "nd theure jurtiente to know maccodlot entoll mond part-tho powlencepisthot in the shepts and a going puisieple wince tt opproald and intule ulfe't int icplet andem ifve them aursifheristly omquis whil\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mwxh, mwhh, mwhy = np.zeros_like(wxh), np.zeros_like(whh), np.zeros_like(why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)\n",
    "smooth_loss = -np.log(1.0/vocab_size) * seq_length\n",
    "while n <= 1000*100:\n",
    "    if (p+seq_length+1 >= len(data)) or (n == 0):\n",
    "        hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "        p = 0 # go  from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "    \n",
    "    loss, dwxh, dwhh, dwhy, dbh, dby, hprev = loss_function(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    \n",
    "    if n % 1000 == 0:\n",
    "        print('Iter ', n, 'loss: ', smooth_loss)\n",
    "        sample(hprev, inputs[0], 200)\n",
    "        \n",
    "    for param, dparam, mem in zip([wxh, whh, why, bh, by], [dwxh, dwhh, dwhy, dbh, dby], [mwxh, mwhh, mwhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "        \n",
    "    p += seq_length # move data pointer\n",
    "    n += 1          # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
